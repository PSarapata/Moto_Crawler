16.12.2020 10:00
----------------
Pushed first stage of my project. As per commit message, I read Scrapy documentation first, then found a nice commercial web scraping project example (real estate)
and based my first Spider on this code. Currently this spider feeds on input.json file, reads through the brand/model dictionary items and scrapes the car selling
website for offers. It then creates an output .json file with each car details.

@update:
---------------
Second Spider crawls another website, which consists of own and external offers. There is a small bug with extracting two feature keys. Will continue working on this after REST API workshop.

20.12.2020 21:00
---------------
Current functionality: Start each spider manually from console. Spiders scrape through 4 popular car selling websites based on input json file, return ready-to-process dictionary items in a json output file. I plan to add one more spider, add pagination for all, then move on to handle PostgreSQL database.

22.12.2020
--------------
Spiders scrape 5 websites, pagination works. There is a unique output file produced for each spider.

29.12.2020 14:30
---------------
I am currently watching tutorials to expand my knowledge on Django REST framework and API building. I might potentially build a full-stack app with a simple React front-end. After I've finished with this, I will move on to Celery tutorials and then hopefully add some code again. Apologies for being quiet. :)

02.01.2021 19:00
---------------
Api is now configured, spiders can now be queued by one-line of code, simply start run.py from terminal. At the
moment pipeline is producing only Offer items, I have already prepared ground for the full relation. After photos, I
will move on to integrating front end and lastly set up celery to run spiders automatically at given times of day.
I will then look to expand the models to add another relation (extra features/information).

03.01.2021 18:40
---------------
I have configured Django ORM to serve Scrapy pipeline, so that it now scans database for duplicates before saving a
db object instance.

API appears to be fully functional. Under /api/ uri there is basic info about each offer,
/api/id/ is the detail retrieve/delete, /api/photos/ stores all photo instances, whereas
/api/offerphoto/id/ is the list of all offerphoto instances <u>for the given offer</u>.

I will now try to add a React Front End and then attempt to automate scraping further, so that instead of firing off
spiders manually from script, the script would fire at specified times of day.

04.01.2021 21:00
---------------
React Front End added, correctly connects through django-cors-headers, fetches data from first page of API (100
records) and displays it onto front page. I modified free Album template from Materials-UI. I also added another
readonly field to Offer serializer to get all URLs for related many-to-many offerphoto instances. Currently the app
looks like this:
                                    https://snipboard.io/u64yJj.jpg

06.01.2021 18:40
---------------
Authentication partially implemented. Once it is fully operational, all views (besides register and login) are
going to be restricted. Until then I keep them as IsAuthenticatedOrReadOnly. JWT tokens are being issued, although
something is still missing. I will try to resolve this within next couple of days.

08.01.2021 18:20
--------------
Authentication finished. Simple search by model name added and tested. Logged in username is shown in the header
element of React. Next up is pagination. I will also implement a relation between user and offers to store User's
favourite ads.

09.01.2021 16:25
--------------
Relation has been added and simple view with a basic serializer also, sitting at
"/api/user/favourites/<int:user_id>/" endpoint. I also realized that offer-photo relation is actually one to many
(one offer can have multiple photos, but each photo can only have one offer), so updated the models accordingly. I
will try to extend favourite offer functionality. After that I will add unit tests in pytest, and also update
documentation throughout the project.
Next up, if time permits, I might attempt to add a notification whenever a specific item appears in the database.

13.01.2021 22:00
--------------
Slight progress - had a brainstorm on how to approach building the favourite user offer functionality.
Decided to go with DEPTH attribute in serializer. View is locked for owner's eyes only.
Still need to figure out a way to Create, Update & Delete related favourite_offer instances.

14.01.2021 15:25
---------------
Major success - using a custom ModelViewSet I got a simple 'favourites' endpoint to list all Offers that were saved
by the user. Passing a PK gives you access to edit (delete) a particular relation. Access is limited to the owner user.
I have tested this using Postman and it actually works as expected! I can now focus on completing documentation for
the project and write some tests. Once I'm done with that, I will finish React side of things.

15.01.2021 20:03
----------------
Developer's journal added along with some docstrings. Depending how much time I can salvage I should finish
project's documentation and unit tests in the next few days, then move back on to React and implement Favourite offers.
(Back End API already has full functionality on those).

17.01.2021 00:20
----------------
 Some more docstrings, first tests, and docs catalogue were added. I am slowly building up projects' documentation there
 with the help of Sphinx theme. You can have a read through the docs by firing off

 "firefox _build/html/index.html"

from terminal, while in /docs directory.

17.01.2021 14:45
----------------
Documentation finished. You can view it using the command above. It may actually come in handy, be sure to try it out!

18.01.2021 19:40
----------------
Tests covering most important functionalities have just been added. Tests rely on custom-made fixtures, populating test
database with fake data, therefore they provide quite complex checks.

19.01.2021 21:30
----------------
Moved docs/ directory to the highest level of the app, hoping to host documentation on GitHub Pages. Fingers crossed. 

25.01.2021 21:45
----------------
Front End is now integrated and handles deleting, adding and dropping "Favourite" status (relation) with a click of a
button. Next up I will attempt to integrate spiders and run them at given times of the day.

26.01.2021 19:35
----------------
Just added Front End pagination for homepage view. It defaults to 50 offers p/page, so it's independent from API
pagination (100 items). Also changed data flow to use Axios with async for fetching paginated offers,
thanks to which I said goodbye to 3-file data handling. Yay!

27.01.2021 00:35
----------------
Implemented working carousel in favourite view. Tomorrow gonna spread it around to offers & search. Also found an article about conditional rendering of favourite status, should be able to render it on relations soon.
